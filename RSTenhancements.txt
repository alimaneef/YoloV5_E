Approach 1: Expanding PED Database Schema
Overview
This approach involves modifying the existing PromotionExecutionDetails (PED) DynamoDB table to store additional details required for tracking partial cashback issuance in retail scenarios. Since PED currently stores promotions at a high level (promotion-wise), it overwrites reward issuance details when multiple shipments contribute to cashback issuance. By restructuring the schema, we can store detailed reward issuance records without overwriting previous ones.

This requires changes to:

Database Schema: New attributes must be added to PED to support granular reward tracking.

DAO Layer: Since the PED structure changes, all Java services interacting with the database will need updates to their DAO (Data Access Object) layer.

GPED API & Other APIs: Any API consuming PED data will need to be modified to interpret the new schema.

Backfilling Strategy: Existing PED records must be migrated to the new format.

Pros of Approach 1
‚úÖ Lower Latency ‚Äì Since all data is stored in a single table, queries are faster as they only involve a single DynamoDB lookup instead of multiple calls across different databases. This reduces API response times.

‚úÖ Simple Querying ‚Äì Fetching data remains simple because all required fields exist within PED, avoiding the need for joins across multiple data sources. Queries remain optimized, and filtering or searching within a single table is efficient.

‚úÖ Easier to Cache at DB Level ‚Äì With all data in one place, read-heavy queries can be optimized using DynamoDB‚Äôs built-in caching solutions like DAX (DynamoDB Accelerator).

‚úÖ Reduced Network Bandwidth Usage ‚Äì Since data is pre-aggregated in a single location, requests don‚Äôt need to fetch data from multiple sources, reducing API call overhead and network latency.

Cons of Approach 1
‚ö†Ô∏è Schema Changes Require Service-Wide Updates ‚Äì Since the structure of PED is modified, all existing APIs, services, and consumers using PED need to be updated. This increases development effort and potential regression issues.

‚ö†Ô∏è Backfilling Overhead ‚Äì All historical records must be migrated to the new schema, requiring a data backfill strategy. This increases risk since incorrect migration could lead to data loss or inconsistencies.

‚ö†Ô∏è Data Overwrites Still Possible ‚Äì Even with schema improvements, if updates are not properly designed, previous issuance records may still get overridden, leading to data integrity issues.

‚ö†Ô∏è Storage Limitations in DynamoDB ‚Äì Each record in DynamoDB has a 400 KB limit. Storing detailed per-shipment cashback data might cause some promotions to exceed this limit, making the solution unsustainable.

‚ö†Ô∏è Less Scalability ‚Äì The PED table would grow significantly in size, and since DynamoDB costs scale with storage and read/write units, this could increase costs compared to a distributed database design.

‚ö†Ô∏è Query Performance Degradation Over Time ‚Äì As data size increases, indexes may become inefficient, leading to higher read costs and slower lookups.

Mitigation Strategies for Approach 1
Compression to Reduce Storage Impact: Data can be compressed before storing it in PED (e.g., Gzip or Brotli compression). However, this introduces another issue‚Äîcompressed data cannot be queried directly, requiring decompression before reading, which increases latency.

API Versioning for Backward Compatibility: Since this approach requires changes to PED‚Äôs data structure, API versioning (query parameter versioning) can ensure existing consumers continue using v1, while the updated API supports v2 with the new schema.

Backfilling with Progressive Migration: Instead of bulk updates, incrementally migrate old records to the new schema to avoid downtime and unexpected failures.

Approach 2: Aggregating Data from Multiple Databases
Overview
Instead of modifying PED, this approach aggregates data from multiple existing databases, including LineItemsBenefitTracker (which stores item-level cashback issuance data). The GetPromotionExecutionDetails (GPED) API would be updated to fetch data dynamically from multiple sources.

This requires changes to:

GPED API Request/Response Model: Since data now comes from multiple databases, the API must be updated to include new response fields.

Parallel Data Fetching: Queries must retrieve data from PED, LineItemsBenefitTracker, and potentially other sources in parallel.

Caching Strategy: Since fetching from multiple sources increases latency, caching must be implemented strategically.

Fallback Mechanisms: If one data source is temporarily unavailable, the system should return partial results instead of failing entirely.

Pros of Approach 2
‚úÖ No Schema Changes to PED ‚Äì Since the database structure remains untouched, existing APIs depending on PED remain functional, reducing migration risks.

‚úÖ Better Data Integrity ‚Äì Since each source owns its own data, there is no risk of overwriting previously issued cashback records, ensuring more accurate tracking.

‚úÖ Scalability & Flexibility ‚Äì The system can dynamically query relevant data sources, making it future-proof for additional use cases. If new cashback issuance sources arise, they can be integrated without modifying existing schemas.

‚úÖ Distributed Storage Reduces DynamoDB Size Limits ‚Äì Since PED is no longer the sole storage for cashback issuance records, DynamoDB‚Äôs 400 KB record limit is no longer a bottleneck.

‚úÖ Supports API Versioning Smoothly ‚Äì A new v2 API can be introduced while maintaining v1 for backward compatibility, ensuring that existing services remain unaffected.

‚úÖ Optimized Querying with Caching ‚Äì Since some data (like eligibility results) is frequently accessed, caching can minimize redundant queries to multiple databases.

Cons of Approach 2
‚ö†Ô∏è Higher Latency Due to Multiple Queries ‚Äì Since data is fetched from multiple sources, the response time is slightly higher than a single-table lookup.

‚ö†Ô∏è Increased Network Bandwidth Usage ‚Äì More data flows between services, increasing network costs, especially if multiple calls are required for a single request.

‚ö†Ô∏è Complex Implementation ‚Äì This approach requires implementing:

Parallel fetching mechanisms to gather data efficiently.

Caching layers to reduce redundant queries.

Fallback mechanisms in case any data source fails.

Mitigation Strategies for Approach 2
Parallel Fetching with Aggregation Timeout ‚Äì Implement async calls with a response aggregator to ensure all data sources return before sending the final API response.

Selective Caching to Reduce Latency ‚Äì Frequently queried data should be cached at the API gateway or database level to reduce redundant fetch operations.

Fallback Mechanisms to Handle Failures ‚Äì If a data source (e.g., LineItemsBenefitTracker) is unavailable, the API can return partial results from PED instead of failing completely.

Final Recommendation: Approach 2 (Multiple DB Aggregation)
üöÄ Why Approach 2 is Better:

Ensures data accuracy (no overwrites)

More scalable and future-proof

No need to modify PED schema (avoiding disruptions to existing services)

Backward compatibility is easier with API versioning

‚ö†Ô∏è Trade-Offs:

Slightly higher latency (mitigated with caching and parallel fetching)

Increased network calls (optimized using selective querying)

Given that our team owns all the databases, Approach 2 provides the most flexibility while ensuring data integrity.


If a new cashback issuance system is introduced later, it can simply be integrated as another data source without impacting PED or breaking existing APIs.
One of the biggest downsides of Approach 1 is that schema changes in PED introduce long-term rigidity. Any future modification to cashback tracking would require further schema redesigns, backfilling, and API migrations, making iterative improvements time-consuming and high-risk
In contrast, Approach 2 is modular‚Äînew data sources can be added without modifying existing storage structures.
With versioning, older clients continue using v1, while new clients gradually migrate to v2 at their own pace.
v1 (Minimal Data) ‚Üí Ideal for teams that only need top-level promotion execution details.
v2 (Enhanced Data) ‚Üí Fetches detailed issuance history from multiple DBs.

***************************************************************
Issue with LastUpdatedTime in PED Queries
Currently, the Rewards Support Tool queries Promotion Execution Details (PED) using either:

BusinessReferenceId, or

CustomerId with a time-based filter (e.g., transactions within a specific period).

However, PED only stores lastUpdatedTime, which creates a problem in scenarios where multiple rewards are associated with a transaction.

Problem Scenario
Consider a transaction where a customer receives three benefits:

One instant cashback

Two scratch cards

Each time the customer redeems a scratch card, the PED record updates, modifying the lastUpdatedTime.

For example:

The customer scratches the first scratch card at 9 AM today ‚Üí lastUpdatedTime updates to 9 AM today.

The customer scratches the second scratch card the next day ‚Üí lastUpdatedTime updates to the next day.

Now, if a support agent queries using customerId and transactionTime within a short time range, including only the transaction timestamp, the tool fails to retrieve the record because the lastUpdatedTime has moved forward (due to the scratch card redemption).

Proposed Solution
Instead of relying on lastUpdatedTime, the PED database should store:

Reward Creation / Reserve Time ‚Äì The time when the reward (cashback or scratch card) was initially issued.

Issuance Time ‚Äì The time when the reward was actually processed and granted to the customer.

By querying based on creation/reserve time rather than lastUpdatedTime, the support tool can correctly retrieve all rewards associated with a transaction, even if their statuses were updated later.

###################################################
Issue with GC ID Overwrites in PED for Retail Events
For retail promotions, cashback is issued incrementally across multiple shipments. Consider a ‚Çπ100 cashback promotion where a customer purchases 5 items:

‚Çπ15 cashback is issued after the first order‚Äôs shipment.

‚Çπ21 cashback is issued after the second order‚Äôs shipment, and so on.

Each cashback issuance generates a concession event, which updates PED with:

benefitIssuanceStatus

benefitIssuanceTime

GC ID (Gift Card ID)

Problem
Since PED stores data at the promotion level, each update overwrites the previous GC ID and issuance details, causing loss of past GC records.

Solution
PED should retain all issued GC IDs instead of overwriting them. Possible approaches:

Store GC IDs as a list/array to track multiple issuances or store them as a Delimited string in PED

##########################
Enhancing Cashback Visibility in the Support Tool
Current Limitation
The support tool currently displays only the total benefit value of a promotion, without reflecting cashback issued in installments or splits. This can cause confusion for support agents, as they cannot see how much cashback has been issued at different stages of the fulfillment process.

Proposed Improvement
Introduce a dynamically updating field that aggregates cashback issued from Concessions DB in real-time. This will ensure that the support tool reflects:

The total eligible benefit under the promotion.

The amount already granted in past installments.

The remaining cashback yet to be issued, if applicable.
###################################

AI-Powered Query Resolution
Planned Integration
We aim to integrate an AI agent to assist in resolving customer queries more efficiently. This agent will:

Provide faster query resolution for customer support agents.

Potentially be extended for direct customer use in the future, with security guardrails in place to restrict sensitive information access.

How It Works
The AI agent will leverage the same API that powers the Rewards Support Tool front end, ensuring consistency in data retrieval. By intelligently gathering relevant information, it will enable:

Quicker issue resolution for support teams.

Scalable customer assistance, reducing manual intervention.

Secure and controlled responses, aligning with compliance and data protection standards.
